# ============================================
# ClaudeNest Environment Configuration
# ============================================

# Application
APP_NAME=ClaudeNest
APP_ENV=local
APP_KEY=
APP_DEBUG=true
APP_URL=http://localhost:8000
APP_TIMEZONE=UTC

# Database (PostgreSQL)
DB_CONNECTION=pgsql
DB_HOST=localhost
DB_PORT=5432
DB_DATABASE=claudenest
DB_USERNAME=claudenest
DB_PASSWORD=claudenest

# Redis
REDIS_HOST=localhost
REDIS_PASSWORD=null
REDIS_PORT=6379

# WebSocket (Laravel Reverb)
REVERB_APP_ID=claudenest
REVERB_APP_KEY=local
REVERB_APP_SECRET=secret
REVERB_HOST=localhost
REVERB_PORT=8080
REVERB_SCHEME=http

# ============================================
# AI Models (Ollama)
# ============================================

# Ollama server URL
OLLAMA_HOST=http://localhost:11434

# LLM for summarization/compression
# Options: mistral:7b, llama3.1:8b, phi3:3.8b, qwen2.5:7b
OLLAMA_MODEL=mistral:7b

# Embedding model for RAG
# Options: bge-small-en-v1.5 (384d), nomic-embed-text (768d)
EMBEDDING_MODEL=bge-small-en-v1.5

# Embedding dimensions (must match model)
# bge-small-en-v1.5 = 384
# nomic-embed-text = 768
EMBEDDING_DIMENSIONS=384

# Reranker model (optional, for better RAG precision)
# RERANKER_MODEL=bge-reranker-base

# Ollama performance settings
# Limit threads if needed (auto-detected if not set)
# OLLAMA_NUM_THREADS=8

# ============================================
# OAuth Providers (Optional)
# ============================================

# Google OAuth
GOOGLE_CLIENT_ID=
GOOGLE_CLIENT_SECRET=
GOOGLE_REDIRECT_URI=${APP_URL}/auth/google/callback

# GitHub OAuth
GITHUB_CLIENT_ID=
GITHUB_CLIENT_SECRET=
GITHUB_REDIRECT_URI=${APP_URL}/auth/github/callback

# ============================================
# Email (for Magic Link auth)
# ============================================

MAIL_MAILER=smtp
MAIL_HOST=smtp.mailgun.org
MAIL_PORT=587
MAIL_USERNAME=
MAIL_PASSWORD=
MAIL_ENCRYPTION=tls
MAIL_FROM_ADDRESS=noreply@claudenest.dev
MAIL_FROM_NAME="${APP_NAME}"

# ============================================
# Multi-Agent Settings
# ============================================

# Default max context tokens per project
DEFAULT_MAX_CONTEXT_TOKENS=8000

# Auto-summarize when context exceeds this threshold (0.0 - 1.0)
CONTEXT_SUMMARIZE_THRESHOLD=0.8

# Context retention days
CONTEXT_RETENTION_DAYS=30

# Task timeout (minutes)
TASK_TIMEOUT_MINUTES=60

# File lock timeout (minutes)
FILE_LOCK_TIMEOUT_MINUTES=30

# ============================================
# Security
# ============================================

# Session lifetime (minutes)
SESSION_LIFETIME=120

# Rate limiting (requests per minute)
RATE_LIMIT_PER_MINUTE=60

# API token expiration (days)
API_TOKEN_EXPIRATION=30

# ============================================
# Logging
# ============================================

LOG_CHANNEL=stack
LOG_LEVEL=debug

# ============================================
# Queue & Cache
# ============================================

QUEUE_CONNECTION=redis
CACHE_STORE=redis
CACHE_PREFIX=claudenest_cache

# ============================================
# Development Only
# ============================================

# SANCTUM_STATEFUL_DOMAINS=localhost,localhost:3000,127.0.0.1,127.0.0.1:8000,::1

# ============================================
# Production Deployment Notes
# ============================================
# 
# For bare-metal deployment:
# 1. Copy this file to .env
# 2. Set secure passwords
# 3. Configure OAuth providers
# 4. Set APP_ENV=production
# 5. Set APP_DEBUG=false
# 6. Generate APP_KEY: php artisan key:generate
#
# For Docker deployment:
# 1. Use docker-compose.prod.yml
# 2. Set DB_PASSWORD and APP_KEY in environment
# 3. Configure OAuth in .env file
#
# AI Models (Ollama) will be automatically pulled on first run
# First download takes time:
#   - mistral:7b ~ 4.4 GB
#   - bge-small-en-v1.5 ~ 130 MB
#
# ============================================
